{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c900134e",
   "metadata": {},
   "source": [
    "### Modelos de regresión lineal\n",
    "\n",
    "La regresión lineal es un modelo matemático que se utiliza para predecir una variable continua a partir de otra variable continua.\n",
    "\n",
    "Vamos a tener el siguiente modelo, un modelo que va a predecir la estatura de una persona a partir de su peso y el peso de sus padres.\n",
    "\n",
    "![Modelo](image.png)\n",
    "\n",
    "El modelo a predecir es:\n",
    "donde:\n",
    "- $y$ es la estatura de la persona\n",
    "- $k$ es el peso de la persona\n",
    "- $p$ es el peso de los padres de la persona\n",
    "- $\\beta_0$ es el intercepto\n",
    "- $\\beta_1$ es la pendiente de la relación entre la estatura y el peso de la persona\n",
    "- $\\beta_2$ es la pendiente de la relación entre la estatura y el peso de los padres de la persona\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 k + \\beta_2 p\n",
    "$$\n",
    "\n",
    "Solo con esto NO PODEMOS hacer el calculo exacto de la estatura de las personas. Por lo que agregamos un residuo $\\epsilon$ que va acumular los residuos y errores.\n",
    "\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 k + \\beta_2 p + \\epsilon\n",
    "$$\n",
    "\n",
    "Los datos son los siguientes\n",
    "\n",
    "| Estatura (y) | Estatura papas (k) | Peso persona (p) |\n",
    "|--------------|------------------|-----------------|\n",
    "| 175          | 70               | 177             |\n",
    "| 181          | 86               | 190             |\n",
    "| 159          | 63               | 180             |\n",
    "| 165          | 62               | 172             |\n",
    "\n",
    "\n",
    "\n",
    "Lo cual serian las siguientes ecuaciones:\n",
    "\n",
    "$$\n",
    "175 = \\beta_0 + \\beta_1 70 + \\beta_2 177\n",
    "$$\n",
    "$$\n",
    "181 = \\beta_0 + \\beta_1 86 + \\beta_2 190\n",
    "$$\n",
    "$$\n",
    "159 = \\beta_0 + \\beta_1 63 + \\beta_2 180\n",
    "$$\n",
    "$$\n",
    "165 = \\beta_0 + \\beta_1 62 + \\beta_2 172\n",
    "$$\n",
    "\n",
    "Esto es un sistema de ecuaciones expresado en matrices de la siguiente forma\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 70 & 177 \\\\\n",
    "1 & 86 & 190 \\\\\n",
    "1 & 63 & 180 \\\\\n",
    "1 & 62 & 172 \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\beta_2 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "175 \\\\\n",
    "181 \\\\\n",
    "159 \\\\\n",
    "165 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "Donde  nos queda una ecuacion\n",
    "\n",
    "$$\n",
    "X \\boldsymbol{\\beta} = \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "\n",
    "EL cambiar de ecuaciones a matrices es muy comun en machine learning.\n",
    "\n",
    "Para que sirve todo esto? \n",
    "Para obtener los coeficientes de la regresion lineal. que nos permitan predecir la estatura de una persona a partir de su peso y el peso de sus padres.\n",
    "\n",
    "Pero esta matriz no es cuadrada, por lo que no podemos sacar la inversa. Las inversas solo se pueden sacar si la matriz es cuadrada y de rango completo. Se considera de rango completo si todas sus columnas son linealmente independientes (rango completo por columnas).\n",
    "\n",
    "## Inversas de matrices rectangulares\n",
    "La inversa de una matriz rectangular, se puede obtener con la inversa por la izquierda y por la derecha.\n",
    "Pero lo que mas se hace es obligar a una matiz rectangular a ser CUADRADA.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "A\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "m x n -> n x m\n",
    "Nesesitamos que tenga mas filas que columnas, es una matriz alta.\n",
    "\n",
    "Como obligamos a que sea cuadrada esta matriz?\n",
    "\n",
    "Si multiplicamos esta matriz por su transpuesta, obtendremos una matriz cuadrada.\n",
    "\n",
    "$$\n",
    "A^T A\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " \n",
    "\\end{bmatrix}\n",
    "\n",
    "n x m  ->  m x n = n x n\n",
    "$$\n",
    "una matriz de n filas por n columnas\n",
    "\n",
    "Pero\n",
    "$$\n",
    "(A^T A)^-1 (A^T A) = I\n",
    "$$\n",
    "\n",
    "Tenemos que  es la matriz inverza izquierda\n",
    "$$\n",
    "(A^T A)^-1 A^T \n",
    "$$\n",
    "![funciona si](image1.png)\n",
    "![funciona si](image2.png)\n",
    "\n",
    "### Matriz inversa por la derecha\n",
    "\n",
    "![matriz ancha](image3.png)\n",
    "![matriz ancha](image4.png)\n",
    "![matriz ancha](image5.png)\n",
    "![inversa por derecha](image6.png)\n",
    "\n",
    "La inversa es derecha si el rango de la matriz es completo en las filas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0e5ac",
   "metadata": {},
   "source": [
    "### DESPEJE DE REGRESION LINEAL\n",
    "\n",
    "Tenemos las siguiente ecuaciones que represntan el sistema a resolver\n",
    "![matrices ecuaciones](image7.png)\n",
    "\n",
    "Donde tenemos X que aplica transformacion al vector Betha, y obtenemos el vector Y de soluciones.\n",
    "![transformacion](image8.png)\n",
    "Al ser una matriz alta para dezpejar debo multiplicar ambos lados por la matriz inversa izquierda de X.\n",
    "Despues simplicando obtenemos\n",
    "\n",
    "![despeje](image9.png)\n",
    "\n",
    "Esto se le conoce como la solucion de minimos cuadrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc77ca45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  70, 177],\n",
       "       [  1,  86, 190],\n",
       "       [  1,  63, 180],\n",
       "       [  1,  62, 172]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## implementacion de la solucion en python seria \n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 70, 177], [1, 86, 190], [1, 63, 180], [1, 62, 172]])\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e0b52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[175],\n",
       "       [181],\n",
       "       [159],\n",
       "       [165]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.array([[175, 181, 159, 165]]).T\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab40514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.08428076,  -5.28145152, -15.91940955,  12.11658031],\n",
       "       [  0.04275543,   0.03147948,  -0.09755097,   0.02331606],\n",
       "       [ -0.07142058,   0.0184702 ,   0.12807992,  -0.07512953]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz inverza\n",
    "x_invizq = np.linalg.inv(X.T @ X) @ X.T\n",
    "x_invizq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eae71ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -2.66204836e-11, -6.82760515e-11],\n",
       "       [-3.05311332e-16,  1.00000000e+00, -1.55486735e-13],\n",
       "       [ 7.91033905e-16,  1.12659881e-13,  1.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Comprobacion matriz identidad\n",
    "x_invizq @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6e49cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[276.85604056],\n",
       "       [  1.51653185],\n",
       "       [ -1.18716219]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betha = x_invizq @ Y\n",
    "betha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf74ff4",
   "metadata": {},
   "source": [
    "Esto datos o variables independientes no son suficientes para predecir la variable dependiente. Por lo mismo los datos de tu variable dependiente, No van a estar en el espacio de columnas de tu matriz de datos independientes. La solucion para esto es aceptar que no podemos modelar con precision el mundo. Simepre vamos a vivir en el error, por lo que a y le vamos a agregar un error con lo que no pudimos precir.\n",
    "![error](image10.png)\n",
    "Esto es la proyeccion ortogonal, donde se busca minimizar el error a lo menos posible.\n",
    "Con esto la meta es encontrar la combinacion de regresores que se acerquen lo mas posible a los datos observados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328af5d7",
   "metadata": {},
   "source": [
    "### Interpretacion geometrica\n",
    "Sabemos que la transformacion lineal X por Betha, forma un plano, del cual tenemos que la variable dependiente y nunca llegara a ser una combinacion lineal del plano.\n",
    "![plano](image11.png)\n",
    "Lo que debemos encontrar son los coeficientes de Betha, que nos de un plano que se acerque lo mas posible a los datos observados, es decir al vector Y.\n",
    "![plano](image12.png)\n",
    "La proyeccion de Y sobre el plano o la distancia,es el vector E, de los erorres.\n",
    "![error](image13.png)\n",
    "Esta distancia mas corta esta dada por la proyeccion ortogonal de Y sobre el plano.\n",
    "![error](image14.png)\n",
    "Donde E es Y menos el X por Betha.\n",
    "\n",
    "Despejando tendremos que:\n",
    "![error](image15.png)\n",
    "Esto seria la solucion de los minimos cuadrados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782aee81",
   "metadata": {},
   "source": [
    "## Minimos cuadrados\n",
    "\n",
    "Para obtener la distancia de los puntos observados a la recta estimada, tenemos que la distancia de cada punto a la recta, que puede ser positivo o negativo.\n",
    "![error](image16.png)\n",
    "Si sumamos las distancias al cuadrado, obtendremos la distancia de los puntos observados a la recta estimada, esta suma seria el error cuadrado.\n",
    "![error](image17.png)\n",
    "\n",
    "La meta es minimizar el error a los minimos cuadrados.\n",
    "\n",
    "Nosotros no podemos construir un super modelo que estime todas las variables independietes posibles para estimar la variable dependiente. Por lo que todos esos errors los podemos capturar en un solo vector:\n",
    "![error](image18.png)\n",
    "\n",
    "Recordar que no solo podemos minimizar los errores tenemos que elevarlos al cuadrado primero.\n",
    "![error](image19.png)\n",
    "\n",
    "Despues de eso lo podemos ver como un problema de optimizacion.\n",
    "![error](image20.png)\n",
    "\n",
    "La inversa izquierda fue buena pero inestable numericamente, porlo que lo mejor es obtar por QR decomposition.\n",
    "![error](image21.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edaa3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz X^T * X:\n",
      "[[     4    281    719]\n",
      " [   281  20109  50734]\n",
      " [   719  50734 129413]]\n",
      "Vector X^T * Y:\n",
      "[[   680]\n",
      " [ 48063]\n",
      " [122365]]\n",
      "Matriz L:\n",
      "[[ 1.          0.          0.        ]\n",
      " [ 0.39082058  1.          0.        ]\n",
      " [ 0.00556328 -0.00443802  1.        ]]\n",
      "Matriz U:\n",
      "[[ 7.19000000e+02  5.07340000e+04  1.29413000e+05]\n",
      " [ 0.00000000e+00  2.81108484e+02  1.56735744e+02]\n",
      " [ 0.00000000e+00  0.00000000e+00 -2.65460105e-01]]\n",
      "Vector V_P: [[122365.]\n",
      " [ 48063.]\n",
      " [   680.]]\n",
      "Paso 1 - Resolver Ly = V_P usando scipy:\n",
      "y = [[1.22365000e+05]\n",
      " [2.40239221e+02]\n",
      " [3.15144199e-01]]\n",
      "Paso 2 - Resolver Ux = y usando scipy:\n",
      "x = [[276.85604056]\n",
      " [  1.51653185]\n",
      " [ -1.18716219]]\n",
      "Paso 3 - Solución final (coeficientes beta):\n",
      "beta = [[276.85604056]\n",
      " [  1.51653185]\n",
      " [ -1.18716219]]\n"
     ]
    }
   ],
   "source": [
    "# SOLUCIÓN CORREGIDA USANDO SCIPY:\n",
    "import numpy as np\n",
    "import scipy.linalg as sp\n",
    "\n",
    "# Datos originales\n",
    "X = np.array([[1, 70, 177], [1, 86, 190], [1, 63, 180], [1, 62, 172]])\n",
    "Y = np.array([[175, 181, 159, 165]]).T\n",
    "\n",
    "# Para mínimos cuadrados necesitamos resolver: (X^T * X) * beta = X^T * Y\n",
    "XTX = X.T @ X  # Matriz cuadrada 3x3\n",
    "XTY = X.T @ Y  # Vector lado derecho\n",
    "\n",
    "print('Matriz X^T * X:')\n",
    "print(XTX)\n",
    "print('Vector X^T * Y:')\n",
    "print(XTY)\n",
    "\n",
    "# Ahora sí podemos aplicar descomposición LU a la matriz cuadrada\n",
    "P, L, U = sp.lu(XTX)\n",
    "print('Matriz L:')\n",
    "print(L)\n",
    "print('Matriz U:')\n",
    "print(U)\n",
    "\n",
    "# Aplicar permutación al vector del lado derecho\n",
    "V_P = P @ XTY\n",
    "print('Vector V_P:', V_P)\n",
    "\n",
    "# Paso 1: Resolver Ly = V_P usando scipy.linalg.solve_triangular\n",
    "y = sp.solve_triangular(L, V_P, lower=True)\n",
    "print('Paso 1 - Resolver Ly = V_P usando scipy:')\n",
    "print('y =', y)\n",
    "\n",
    "# Paso 2: Resolver Ux = y usando scipy.linalg.solve_triangular\n",
    "x = sp.solve_triangular(U, y, lower=False)\n",
    "print('Paso 2 - Resolver Ux = y usando scipy:')\n",
    "print('x =', x)\n",
    "\n",
    "print('Paso 3 - Solución final (coeficientes beta):')\n",
    "print('beta =', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05158c2",
   "metadata": {},
   "source": [
    "## Resumen:\n",
    "\n",
    "-   El modelo de regresion lineal es un modelo estadistico para entender nuestro mundo.\n",
    "-   Se habla diferente en Estadistica que en Algebra Lineal.\n",
    "-   El Metodo del Minimos Cuadrados via Inversa es el cimiento de muchos analisis estadisticos.\n",
    "-   La Formula de Minimos cuadrados puede ser entendida usando algebra lineal, geometria o calculo.\n",
    "-   Aunque en teoriaca deberiamos multiplicar el vector de datos por su inversa izquierda, en la practica usamos Descomposion QR.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algebra_lineal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
